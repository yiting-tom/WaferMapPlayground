### **Detailed Hyperparameters for MobileNetV3-Large and TinyViT Fine-Tuning**

| Hyperparameter | **MobileNetV3-Large** üì± (CNN) | **TinyViT** üñºÔ∏è (Vision Transformer) | Rationale |
| :--- | :--- | :--- | :--- |
| **Learning Rate** | **Base LR:** $1e-4$ to $5e-5$ | **Base LR:** $1e-4$ to $5e-5$ | A small learning rate is crucial to fine-tune the pre-trained weights without causing large, disruptive changes. Starting with a low rate allows the model to make small, careful steps toward convergence on the new dataset. |
| **Optimizer** | **AdamW** is highly recommended. | **AdamW** is highly recommended. | AdamW is a variant of the Adam optimizer that decouples weight decay from the L2 regularization term. This is especially effective for models with a high number of parameters, as it helps prevent overfitting and improves generalization. |
| **Batch Size** | **32 to 64** | **32 to 64** | The batch size should be large enough to provide a stable gradient signal but small enough to fit in your GPU's memory. This range is a good starting point for a dataset of 1920 images, and can be adjusted based on GPU constraints. |
| **Epochs** | **30 to 50** with **Early Stopping** | **30 to 50** with **Early Stopping** | Given the small dataset, training for too many epochs will inevitably lead to overfitting. Early stopping, where training halts if validation loss doesn't improve for a few epochs, is essential. Set a `patience` of around 5-10 epochs. |
| **Loss Function** | `torch.nn.BCEWithLogitsLoss` (PyTorch) or `tf.keras.losses.BinaryCrossentropy` (TensorFlow) | `torch.nn.BCEWithLogitsLoss` (PyTorch) or `tf.keras.losses.BinaryCrossentropy` (TensorFlow) | This is the go-to loss function for multi-label classification. It computes the binary cross-entropy loss for each of the 8 labels independently, using the model's raw output (logits) for numerical stability. |
| **Head (Classifier)** | Replace the final layer with a **new linear layer** that outputs 8 values. | Replace the final layer with a **new linear layer** that outputs 8 values. | Both models' pre-trained classifiers were designed for a different number of classes (e.g., 1000 for ImageNet). You must replace this with a new layer that maps the model's features to your 8 labels. |
| **Layers to Unfreeze** | **Stage 1:** Freeze all base layers, train only the new head.\<br\>**Stage 2 (Optional):** Unfreeze the last few blocks (e.g., the last `inverted_residual` block) and train the whole model with a tiny learning rate (e.g., $1e-5$ to $1e-6$). | **Stage 1:** Freeze all base layers, train only the new head.\<br\>**Stage 2 (Optional):** Unfreeze the last few transformer blocks and the new head, and train with a tiny learning rate. | This two-stage approach is a best practice for fine-tuning. It starts by training the new head to classify features learned by the pre-trained model and then gradually fine-tunes the base model to better adapt to your specific data. |
| **Learning Rate Scheduler** | **Cosine Annealing with Warmup** | **Cosine Annealing with Warmup** | This scheduler starts with a very low learning rate (`warmup`), gradually increases it, and then slowly decreases it following a cosine curve. This technique is known to lead to better final models by allowing for fine-tuning at the end of the training process.  |
| **Data Augmentation** | **Basic:** Random cropping, horizontal flipping, random rotation (e.g., $\\pm 15^\\circ$), and color jitter.\<br\>**Advanced:** Use libraries like Albumentations for more effective augmentations. | **Basic:** Same as MobileNet.\<br\>**Advanced:** Include **Mixup** and **CutMix**. These techniques are particularly beneficial for Vision Transformers, as they help the model generalize by creating synthetic training samples from linear combinations of existing ones.  | Given the small dataset, aggressive data augmentation is the single most important technique to combat overfitting. It artificially increases the size and diversity of your training data. The specific advanced techniques used for ViTs help them overcome their lack of inherent inductive biases compared to CNNs. |